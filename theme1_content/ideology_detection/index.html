<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.6.3"><meta name=author content="Marian-Andrei Rizoiu"><meta name=description content="Last November 2022, we presented to the [Defence Human Sciences Symposium 2022](https://www.dst.defence.gov.au/event/defence-human-sciences-symposium-2022) our novel ideology detection pipeline. In this post, we explain the technology and the big takeaways."><link rel=alternate hreflang=en-us href=https://www.behavioral-ds.science/theme1_content/ideology_detection/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-983V3P8SQ2"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){document.location=url;}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target);}
gtag('js',new Date());gtag('config','G-983V3P8SQ2',{});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/img/icon-32.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://www.behavioral-ds.science/theme1_content/ideology_detection/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Behavioral Data Science"><meta property="og:url" content="https://www.behavioral-ds.science/theme1_content/ideology_detection/"><meta property="og:title" content="Detecting extreme ideologies in shifting landscapes | Behavioral Data Science"><meta property="og:description" content="Last November 2022, we presented to the [Defence Human Sciences Symposium 2022](https://www.dst.defence.gov.au/event/defence-human-sciences-symposium-2022) our novel ideology detection pipeline. In this post, we explain the technology and the big takeaways."><meta property="og:image" content="https://www.behavioral-ds.science/theme1_content/ideology_detection/featured.png"><meta property="twitter:image" content="https://www.behavioral-ds.science/theme1_content/ideology_detection/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-02-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-02T00:00:00+00:00"><title>Detecting extreme ideologies in shifting landscapes | Behavioral Data Science</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container style=max-width:93%><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/img/logo.png alt="Behavioral Data Science"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/img/logo.png alt="Behavioral Data Science"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class="nav-item dropdown"><a href=/research class=nav-link aria-haspopup=true><span>Research</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/research/#theme1><span>Information spread, influence and attention</span></a>
<a class=dropdown-item href=/research/#theme2><span>Disinformation and online problematic content</span></a>
<a class=dropdown-item href=/research/#theme3><span>The labour markets of tomorrow</span></a></div></li><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/reading><span>Reading</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/behavioral-ds target=_blank rel=noopener><span><i class="fab fa-github" style=color:#333;font-size:1rem></i></span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav><article class=article><div class="article-container pt-3"><h1>Detecting extreme ideologies in shifting landscapes</h1><div class=article-metadata><div><span><a href=/authors/rohit-ram/>Rohit Ram</a></span>, <span><a href=/authors/edited-by-marian-andrei-rizoiu/>(edited by Marian-Andrei Rizoiu)</a></span></div><span class=article-date>Feb 2, 2023</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/categories/research/>Research</a>, <a href=/categories/blogpost/>blogpost</a></span></div></div><div class=article-container><div class=article-style><iframe width=560 height=315 src=https://www.youtube.com/embed/csWMgU7R52Q title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><p>Also check out <a href=../authors/rohit-ram/>Rohit</a>’s and <a href=../authors/ma-rizoiu/>Andrei</a>’s article that just appeared in The Conversation: <a href=https://theconversation.com/can-ideology-detecting-algorithms-catch-online-extremism-before-it-takes-hold-200629>Can ideology-detecting algorithms catch online extremism before it takes hold?</a></p><p>In this <a href=https://arxiv.org/pdf/2208.04097.pdf>our latest working paper</a>, we propose a completely automatic end-to-end ideology detection pipeline for the detection and psychosocial profiling of left-right political ideology, as well as far-right ideological users.
The pipeline fills a crucial gap by providing flexible methodology and tooling for understanding ideologies and building early warning systems for extreme ideology-motivated (and potentially violent) activity.</p><p><strong>Paper citation:</strong></p><pre><code>Ram, R. and Rizoiu, M.A., 2022. You are what you browse: A robust framework for uncovering political ideology.
arXiv preprint arXiv:2208.04097.</code></pre><p>(<em>see full paper here: <a href=https://arxiv.org/pdf/2208.04097.pdf>https://arxiv.org/pdf/2208.04097.pdf</a></em>)</p><div id=ideology-in-an-online-world class="section level2"><h2>Ideology in an Online World</h2><p>Ideology determines how we make sense of much of the world, our opinions, and our political actions.
It is not a new concept; throughout history, it served as the context for unrest.
However, ideological spread and radicalization have entered a new paradigm in our ever-connected world. The internet is a significant source of information and spreads opinions quickly through social platforms.
In particular, the anonymity and lack of accountability often associated with online communication set up a supportive environment for spreading far-right ideologies and radicalizing individuals into extremist groups.
Far-right extremism is a form of ideology that advocates for ultranationalism, racism, and opposition to immigration and multiculturalism.
These ideologies strongly correlate with violence and terrorism and threaten individual and collective security.</p><p>The Australian Security Intelligence Organisation (ASIO) raised concerns about Australians being radicalized very young and the rise of extremist movements in Australia through online technologies <span class=citation>(<a href=#ref-asio role=doc-biblioref><span>“Director-General’s Annual Threat Assessment”</span> 2021</a>)</span>.
ASIO claimed that during the COVID period, 30-40% of their caseload was devoted to far-right extremism, up from 10-15% in 2016 <span class=citation>(<a href=#ref-guardian1 role=doc-biblioref>Karp 2020</a>)</span>.</p><p>Unfortunately, Ideologically Motivated Violent Extremism (IMVE) continues to be an issue in Australia.
On December 12th, 2022, two Queensland police officers were killed while performing routine duties <span class=citation>(<a href=#ref-guardian2 role=doc-biblioref>Gillespie and McGowan 2022</a>)</span>.
Later investigations would uncover that the three people, who killed the officers, were active online in producing deep-state and religious conspiratorial content.
Their content has since been removed from mainstream social platforms but continues to be shared on conspiratorial websites.
Such extreme-leaning content often serves as a lead indicator of violent extremism (as was the case in this incident and the Christchurch Mosque Shootings three years prior). However, the tools to identify and understand the psychosocial characteristics of these extreme individuals and communities are lacking.</p><p>In this work, we build an end-to-end ideology detection pipeline and psychosocial profiles of ideological groups.
We find that right-leaning individuals tend to use moral-vice language more than left-leaning and that far-right individuals’ grievance language (violence, hate, paranoia, etc.) significantly differs from the moderates.</p></div><div id=signals-of-ideology class="section level2"><h2>Signals of Ideology</h2><p>In online social settings, researchers face numerous barriers that prevent using traditional methods.
Directly asking users for their ideologies has dubious success, infringes on platform T&Cs, and does not scale to online populations.
Inferring users’ ideologies from their activity also does not scale as the data requires is prohibitively expensive and tedious to compile.</p><p>Instead, to reduce expert labor to feasible levels, researchers infer ideologies from signals in user behavior – such as whether they use political hashtags, retweet politicians, or follow political parties.
We dub these signals <em>ideological proxies</em>.</p><p>Importantly, these <em>ideological proxies</em> for online users can still require laborious labeling by context-specific experts.
For example, the hashtag <em>#ScottyFromMarketing</em> requires an up-to-date expert in Australian politics to uncover that it expresses an anti-right-wing ideology.
For many researchers:
- access to contextual experts is difficult,
- labeling of signals is still laborious and expensive,
- and context switches require relabelling (exasperating the above problems).</p><p>Unfortunately, such context switches are commonplace, as the context changes with time, country, or social platform.
Figure <a href=#fig:teaser>1</a> showcases the problem: most commonly used <em>ideological proxies</em> can only be transferred in narrow circumstances (represented by the green dotted regions).
For example, following political parties is country-dependent, politicians come and go with time, and hashtags are platform-dependent.
As such, we desire an <em>ideological proxy</em> that is robust to changes in context, requires no expert labeling and is true to the gold standard.</p><div class=figure><span style=display:block id=fig:teaser></span><img src=teaser_v2.svg alt="Schema showing that not all ideological proxies can context-switch." width=700px><p class=caption>Figure 1: Schema showing that not all ideological proxies can context-switch.</p></div><p>Furthermore, the <em>ideological proxies</em> are often sparse among users; however, we would ideally like to detect influence amongst the entire population of users (as taking only active users could bias our inferences).
We further desire a method for inferring the ideology of (potentially inactive) users without direct <em>ideological proxy</em> information.</p></div><div id=our-solution-you-are-what-you-browse class="section level2"><h2>Our Solution: You are what you browse</h2><p>Our solution is a large-scale end-to-end ideology detection pipeline that can be used to profile entire populations of users.
The solution has two main components; the media proxy and the inference architecture.
The media proxy allows for labeling a subset of users, and the inference architecture allows for propagating these labels to the remaining users via socially-informed homophilic lenses.</p><div id=the-media-proxy class="section level3"><h3>The Media Proxy</h3><p>For the first part of our work, we generate a proxy based on media-sharing behavior, which satisfies the desiderata.</p><p>We generate the media proxy via two media slant datasets (although many are widely available).
The first is an extensive survey of media consumption behaviors conducted by Reuters <span class=citation>(<a href=#ref-newman2019reuters role=doc-biblioref>Newman et al. 2019</a>)</span> in several countries in 2020 and 2021.
Participants reported the media publications they consume and their own political ideology.
We estimate the slant of a media source for each country and year as the average ideology of the participants who consume it.
The second dataset is the Allsides Media Bias Dataset <span class=citation>(<a href=#ref-sides2018media role=doc-biblioref>Sides 2018</a>)</span>, which contains an expert-curated set of media publications.
The Allsides dataset contains mostly American-based media; conversely, Reuters covers the major media outlets in each country.
Given that each country and period will have a different conception of ideologies, we calibrate Reuter’s media slants to approximate the Allsides (minimizing the mean-squared error). Figure <a href=#fig:slants>2</a> shows the slants for each media website within the Reuters dataset.</p><div class=figure><span style=display:block id=fig:slants></span><img src=url_slants.svg alt="Plot showing the slants for various media websites." width=700px><p class=caption>Figure 2: Plot showing the slants for various media websites.</p></div><p>Finally, we quantify a user’s ideology as the average ideology of their shared media.</p><p>The media proxy resolves the issue of context switching; since it is applicable across many contexts and can be used widely in a fully automated fashion.
This allows us to create an end-to-end ideology detection pipeline.</p><p>We further define methods to classify far-right users from their media-sharing behaviors, which we fully describe in the paper.</p></div><div id=the-inference-architecture class="section level3"><h3>The Inference Architecture</h3><p>In the second part of our work, we define an inference architecture that allows inferring the ideological labels of the remaining users – e.g., users who do not share any URLs.
Our inference architecture relies on the sociological principle of homophily, where we hypothesize that similar users will share a similar ideology.
We measure homophily through three distinct lenses;</p><ol style=list-style-type:decimal><li><em>Lexical</em>: Users with similar language will have similar ideology</li><li><em>Hashtag</em>: Users who participate in similar topics of discussion share a similar ideology</li><li><em>Resharing</em>: Users who consume similar content (signaled via resharing of other users) will share a similar ideology</li></ol><p>Through these lenses, we utilize an AutoML model, FLAML <span class=citation>(<a href=#ref-wang2021flaml role=doc-biblioref>Wang et al. 2021</a>)</span> (with the LightGBM architecture), trained on users identified via an ideological proxy to propagate the labels to the remaining users and generate a complete ideological profile for a dataset.</p></div></div><div id=the-data class="section level2"><h2>The Data</h2><p>We utilize several large-scale datasets from various platforms to showcase the relative ease of applying our end-to-end pipeline.
The datasets’ characteristics are described in Table <a href=#tab:datasets>1</a>.</p><table><caption><span id=tab:datasets>Table 1: </span>The datasets used through-out the analysis, with the number of users, posts, and affliated country.</caption><thead><tr class=header><th align=left>Dataset</th><th align=left>Users</th><th align=left>Posts</th><th align=left>Country</th></tr></thead><tbody><tr class=odd><td align=left>#Qanda</td><td align=left>103,074</td><td align=left>768,808</td><td align=left>AUS</td></tr><tr class=even><td align=left>#Ausvotes</td><td align=left>273,874</td><td align=left>5,033,982</td><td align=left>AUS</td></tr><tr class=odd><td align=left>#SocialSense</td><td align=left>49,442</td><td align=left>358,292</td><td align=left>AUS</td></tr><tr class=even><td align=left>Riot</td><td align=left>574,281</td><td align=left>1,067,794</td><td align=left>US</td></tr><tr class=odd><td align=left>Parler</td><td align=left>120,048</td><td align=left>603,820</td><td align=left>US</td></tr></tbody></table><div id=psychosocial-profiles-of-the-ideological-groups class="section level3"><h3>Psychosocial profiles of the Ideological Groups</h3><p>Large-scale profiling of entire online populations gives us significant insights into the characteristics of online populations.
We apply our inferred ideological labels of online users in two critical ways:</p><ol style=list-style-type:decimal><li>We derive a method for distinguishing the left and the right in terms of their moral language.</li><li>We derive a method for distinguishing moderate and extreme ideologies in terms of their grievance language.</li></ol><p><strong>Distinguishing Left and Right</strong>:
We utilize the FrameAxis <span class=citation>(<a href=#ref-mokhberian2020moral role=doc-biblioref>Mokhberian et al. 2020</a>)</span> methodology to metricize each user’s association with each of the five Moral Foundations <span class=citation>(<a href=#ref-graham2013moral role=doc-biblioref>Graham et al. 2013</a>)</span> in terms of their vice and virtue axes.
Given the measures of the Moral Foundations in the user language, we can start to detect in what way the left and the right differ.
To do this, we find each ideological group’s mean vice and virtue scores and compare these to the neutral group.
Figure <a href=#fig:mft>3</a> shows the outcome of this analysis on the SocialSense dataset.</p><div class=figure><span style=display:block id=fig:mft></span><img src=mft_diff_plot.svg alt="Plot of the moral foundations of ideological groups in the SocialSense dataset, showing that the left prefer virtue and the right prefer vice language." width=700px><p class=caption>Figure 3: Plot of the moral foundations of ideological groups in the SocialSense dataset, showing that the left prefer virtue and the right prefer vice language.</p></div><p>We see that the left prefers the language of virtue, while the right prefers the language of vice.
This trend is largely consistent across all datasets.</p><p><strong>Distinguishing Moderates and Extremes</strong>: We similarly generate measures via the Grievance Dictionary <span class=citation>(<a href=#ref-van2021grievance role=doc-biblioref>Van der Vegt et al. 2021</a>)</span>, a threat assessment tool designed to highlight potential threats through their language.
Similar to the previous plot, we investigate the distribution of grievance scores for the ideological groups.
However, here we measure the difference between distributions with the Signed KL-divergence (a measure of the difference in the location and shape of distributions).
Figure <a href=#fig:grievance>4</a> shows the results for the Ausvotes dataset.</p><div class=figure><span style=display:block id=fig:grievance></span><img src=grievance_diff_plot.svg alt="Plot of the grievance categories of ideological groups in the #Ausvotes dataset, showing that the far-right is significantly different." width=700px><p class=caption>Figure 4: Plot of the grievance categories of ideological groups in the #Ausvotes dataset, showing that the far-right is significantly different.</p></div><p>We observe that the far-right’s usage of grievance language is significantly different from the moderate ideological groups.
This adds evidence to the growing concern that members of the far-right may vent their frustration and participate in violent behavior.</p></div></div><div id=conclusion class="section level2"><h2>Conclusion</h2><p>In this work, we build a fully automatic end-to-end ideology detection pipeline for left-right and far-right detection.
Importantly, with the pipeline, we can show the differences between the left and right, and moderates and extremes in terms of psychosocial language, across a range of diverse datasets.</p><div id=references class="section level3 unnumbered"><h3>References</h3><div id=refs class="references csl-bib-body hanging-indent"><div id=ref-asio class=csl-entry><span>“Director-General’s Annual Threat Assessment.”</span> 2021. <em>ASIO</em>. <a href=https://www.asio.gov.au/resources/speeches-and-statements/director-generals-annual-threat-assessment-2021>https://www.asio.gov.au/resources/speeches-and-statements/director-generals-annual-threat-assessment-2021</a>.</div><div id=ref-guardian2 class=csl-entry>Gillespie, Eden, and Michael McGowan. 2022. <span>“Queensland Shooting: Gareth and Stacey Train Published YouTube Video After Killing Police Officers.”</span> <a href=https://www.theguardian.com/australia-news/2022/dec/16/queensland-shooting-gareth-and-stacey-train-youtube-video-published-after-killing-police class=uri>https://www.theguardian.com/australia-news/2022/dec/16/queensland-shooting-gareth-and-stacey-train-youtube-video-published-after-killing-police</a>; The Guardian.</div><div id=ref-graham2013moral class=csl-entry>Graham, Jesse, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. <span>“Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism.”</span> In <em>Advances in Experimental Social Psychology</em>, 47:55–130. Elsevier.</div><div id=ref-guardian1 class=csl-entry>Karp, Paul. 2020. <span>“Asio Reveals up to 40% of Its Counter-Terrorism Cases Involve Far-Right Violent Extremism.”</span> <a href=https://www.theguardian.com/australia-news/2020/sep/22/asio-reveals-up-to-40-of-its-counter-terrorism-cases-involve-far-right-violent-extremism class=uri>https://www.theguardian.com/australia-news/2020/sep/22/asio-reveals-up-to-40-of-its-counter-terrorism-cases-involve-far-right-violent-extremism</a>; The Guardian.</div><div id=ref-mokhberian2020moral class=csl-entry>Mokhberian, Negar, Andrés Abeliuk, Patrick Cummings, and Kristina Lerman. 2020. <span>“Moral Framing and Ideological Bias of News.”</span> In <em>International Conference on Social Informatics</em>, 206–19. Springer.</div><div id=ref-newman2019reuters class=csl-entry>Newman, Nic, Richard Fletcher, Antonis Kalogeropoulos, DAL Levy, and Rasmus Kleis Nielsen. 2019. <span>“Reuters Institute Digital News Report 2018. Reuters Institute for the Study of Journalism.”</span> Oxford.</div><div id=ref-sides2018media class=csl-entry>Sides, All. 2018. <span>“Media Bias Ratings.”</span> <em>Allsides. Com</em>. <a href=https://www.allsides.com/media-bias/ratings>https://www.allsides.com/media-bias/ratings</a>.</div><div id=ref-van2021grievance class=csl-entry>Van der Vegt, Isabelle, Maximilian Mozes, Bennett Kleinberg, and Paul Gill. 2021. <span>“The Grievance Dictionary: Understanding Threatening Language Use.”</span> <em>Behavior Research Methods</em> 53 (5): 2105–19.</div><div id=ref-wang2021flaml class=csl-entry>Wang, Chi, Qingyun Wu, Markus Weimer, and Erkang Zhu. 2021. <span>“FLAML: A Fast and Lightweight Automl Library.”</span> <em>Proceedings of Machine Learning and Systems</em> 3: 434–47.</div></div></div></div></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://www.behavioral-ds.science/theme1_content/ideology_detection/&text=Detecting%20extreme%20ideologies%20in%20shifting%20landscapes" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://www.behavioral-ds.science/theme1_content/ideology_detection/&t=Detecting%20extreme%20ideologies%20in%20shifting%20landscapes" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Detecting%20extreme%20ideologies%20in%20shifting%20landscapes&body=https://www.behavioral-ds.science/theme1_content/ideology_detection/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://www.behavioral-ds.science/theme1_content/ideology_detection/&title=Detecting%20extreme%20ideologies%20in%20shifting%20landscapes" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Detecting%20extreme%20ideologies%20in%20shifting%20landscapes%20https://www.behavioral-ds.science/theme1_content/ideology_detection/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://www.behavioral-ds.science/theme1_content/ideology_detection/&title=Detecting%20extreme%20ideologies%20in%20shifting%20landscapes" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="portrait mr-3" src=/authors/rohit-ram/avatar_hu440177783dd1505b8cf08c8d56781e24_11726_250x250_fill_q90_lanczos_center.jpg alt=Avatar><div class=media-body><h5 class=card-title><a href=/authors/rohit-ram/>Rohit Ram</a></h5><h6 class=card-subtitle>PhD student</h6><p class=card-text>Hi, I'm Rohit. I'm a <strong>PhD candidate at the University of Technology Sydney</strong>, interested in how people exhibit opinions and behaviours in online social environments. If you're interested in my research or just want to say &lsquo;hi&rsquo;, <strong>send us a message on <a href=https://twitter.com/rohitram96>Twitter</a></strong>.</p><ul class=network-icon aria-hidden=true><li><a href=https://twitter.com/rohitram96 target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://github.com/rohitram96 target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://enchiridion.ml/ target=_blank rel=noopener><i class="fas fa-user"></i></a></li></ul></div></div></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script id=dsq-count-scr src=https://behavioral-ds.disqus.com/count.js async></script><script src=/js/academic.min.b3ed283fe092cc42617915d1b60d8923.js></script><script src=//yihui.org/js/math-code.js></script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><div class=container><footer class=site-footer><p class=powered-by>2023 &#183;
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>